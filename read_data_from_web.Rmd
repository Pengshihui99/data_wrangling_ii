---
title: "read data from the web"
author: "Shihui Peng"
date: "2023-10-22"
output: github_document
---

```{r, message=FALSE, echo=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r}
library(rvest)
library(httr)
```

* `rvest`: provides tools for web scraping and parsing HTML content. Useful for extracting data from websites. It allows you to navigate and select elements from HTML documents using CSS or XPath selectors.
* `httr`: makes HTTP requests. It provides functions for sending HTTP requests, handling responses, and working with APIs.

# Extracting tables

## Import NSDUH data
```{r}
nsduh_url = 'http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm'

nsduh_html =
  read_html(nsduh_url)

nsduh_html
```

* it looks weird, but we do successfully have an html document. so the next thing is to pull out the elements of this html file tha are actually relevant for what we want.

* Rather than trying to grab something using a CSS selector, let’s try our luck extracting the tables from the HTML.
```{r}
nsduh_html |> 
  html_table()
```

* the problem that goes along w this: it imports every single table. we get 15 tables in this website stored as a list.

* the next step: figure out how to get data that we actually want. E.g., if i only want the first table
```{r}
marj_use_df =
  nsduh_html |> 
  html_table() |> 
  first() |> 
  slice(-1)
```

* `first()`: i want to 1st table
* `slice(-1)`: means minus the 1st row. the first row is the NOTE from the web and it is weird. use this to remove that row. 

# CSS selectors

## Import star wars...

* BUT, we cannot just select tables here. We need to figure out what are the html elements that we want out of that page for Star War.
```{r}
swm_url = 'https://www.imdb.com/list/ls070150896/'

swm_html = 
  read_html(swm_url)
```
we can also do `swm_html = read_html("https://www.imdb.com/list/ls070150896/")`

* the trick now is to get what we want from this html file.
  * i have star war html file now. i need to get some elements by specifying the CSS tag for things i want to get.
  * go to Chrome, click **SelectorGadget** (top right), select stuff needed and remove stuff thai is no need.
    * eg.select the title, and everything selected will turns into yellow. in the bar st the bottom right, it shows 'a', which means everything that is a link is selected. But we just want the title, so we click one of the other stuffs that has a link (that stuff will turn into red, indicating that it has been removed). Then the bottom right is the CSS tag of title, which is what we want. Then we copy it and paste it inside `html_elements()`
```{r}
swm_title_vec =
  swm_html |> 
  html_elements('.lister-item-header a') |> 
  html_text()

swm_gross_revenue_vec =
  swm_html |> 
  html_elements('.text-muted .ghost~ .text-muted+ span') |> 
  html_text()

swm_df =
  tibble(
    title = swm_title_vec,
    gross_rev = swm_gross_revenue_vec
  )
```

* **`html_elements()`**: to gt elements we want from the web. use **SelectorGadget** to get the **CSS tag** and put the CSS tag into this code.
* how do i get stuff out? -- **`html_text`**: make a couple of diff guess for which text in the html piece do you want. so it extracts the text from the html that goes alng w the CSS tag that i want.

# Get data directly from an API

## Get water data from NYC - import as a csv and parse it

```{r}
nyc_water_df = 
  GET('https://data.cityofnewyork.us/resource/ia2d-e54m.csv') |> 
  content('parsed')
```

* **`GET()`**: it is gonna ask for whatever the thing that i'm trying to get. it is like sending a request to API to get the stuff we want.
  * in the NYC web, there is a API button. click it and in the API Endpoint, select CSV (the format we want, JSON if by default), then copy it and paste into `GET()`
* **`content('parsed')`** processes the HTTP response content.
  * **`'parsed'`**: is passed as an argument to content(), instructing httr to automatically parse the content based on its type. In this case, it's a CSV file, so it will be parsed into a data frame.
  
## Get water data from NYC - import as a JSON file

We can also import this dataset as a JSON file. This takes a bit more work (and this is, really, a pretty easy case), but it’s still doable.
```{r}
nyc_water_json_df = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.json") |> 
  content("text") |>
  jsonlite::fromJSON() |>
  as_tibble()
```

## BRFSS data

```{r}
brfss_smart2010 = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv",
      query = list("$limit" = 5000)) |> 
  content("parsed")
```

* By default, the CDC API limits data to the `first 1000 rows`. Here I’ve increased that by changing an element of the API query – I looked around the website describing the API to find the name of the argument (click API, it's in `API Docs`, it may tell us something we may need if we want more info), and then used the appropriate syntax for GET. To get the full data, I could increase this so that I get all the data at once or I could try iterating over chunks of a few thousand rows.
  * but almost every API is different in terms of what we want to request
  
* **`query = list("$limit" = 5000)`**: It includes a query parameter, $limit, with a value of 5000. This parameter limits the number of records returned from the data source.

* why not just download a csv file from web?
  * reproducibility - if other wants to run your code from their end, they can get data in exactly same way as i did w/o transfer a datafile.
  * datafile can be automatically updated every time we run the code corresponding to the web.
  
## Pokemon data

```{r}
poke_df = 
  GET('https://pokeapi.co/api/v2/pokemon/ditto') |> 
  content()
```

* if ended with content(), we will see weird things. so we need to do data organization this df.
  * sometimes API gives us nicely structured thing (as water data and brfss data), but sometimes they don't (as this pokemon data)
  
  
* For both of the API examples we saw today (water ds and brfss ds), it wouldn’t be terrible to just download the CSV, document where it came from carefully, and move on. APIs are more helpful when the full dataset is complex and you only need pieces, or when the data are updated regularly.
